{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from datasets import DatasetDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration stochastic--random_streetview_images_pano_v0.0.2-65d25597b51c2d04\n",
      "Found cached dataset parquet (/Users/derekyu/.cache/huggingface/datasets/stochastic___parquet/stochastic--random_streetview_images_pano_v0.0.2-65d25597b51c2d04/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c659c76fe89147a9a005043edd941c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stochastic/random_streetview_images_pano_v0.0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - this took 6min 48 seconds on my computer... jk 12 min second time\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms as transforms\n",
    "# data pre processing\n",
    "transform = v2.Compose([\n",
    "    v2.CenterCrop((561,1010)), # crops the middle image from the panorama\n",
    "    v2.Resize((224,224)), # resizing the image to 224x224 for easier processing\n",
    "    transforms.ToTensor() # converting the image to a tensor\n",
    "])\n",
    "\n",
    "# Create a new list to hold the transformed dataset\n",
    "transformed_dataset = {'train': []}\n",
    "\n",
    "# Loop through the original dataset and apply the transformation\n",
    "for item in dataset['train']:\n",
    "    transformed_item = item.copy()  # Make a copy of the current item\n",
    "    transformed_item['image'] = transform(item['image'])  # Transform the image\n",
    "    transformed_dataset['train'].append(transformed_item)  # Add to the new dataset\n",
    "\n",
    "# for i in range(len(dataset['train'])):\n",
    "#     dataset['train'][i]['image'] = transform(dataset['train'][i]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5647, 0.6000, 0.6667,  ..., 0.5843, 0.5922, 0.5922],\n",
       "         [0.5804, 0.5255, 0.5255,  ..., 0.5843, 0.5922, 0.5922],\n",
       "         [0.5176, 0.5451, 0.5412,  ..., 0.5843, 0.5922, 0.5922],\n",
       "         ...,\n",
       "         [0.5961, 0.5843, 0.5922,  ..., 0.5765, 0.5725, 0.5569],\n",
       "         [0.5882, 0.6196, 0.6157,  ..., 0.5882, 0.5725, 0.5686],\n",
       "         [0.5804, 0.6431, 0.6118,  ..., 0.5725, 0.5569, 0.5255]],\n",
       "\n",
       "        [[0.5373, 0.5804, 0.6863,  ..., 0.7451, 0.7490, 0.7490],\n",
       "         [0.5529, 0.5020, 0.5373,  ..., 0.7451, 0.7490, 0.7490],\n",
       "         [0.4863, 0.5098, 0.5373,  ..., 0.7451, 0.7490, 0.7490],\n",
       "         ...,\n",
       "         [0.5608, 0.5490, 0.5569,  ..., 0.5451, 0.5490, 0.5294],\n",
       "         [0.5529, 0.5843, 0.5804,  ..., 0.5529, 0.5490, 0.5412],\n",
       "         [0.5490, 0.6078, 0.5725,  ..., 0.5333, 0.5294, 0.4980]],\n",
       "\n",
       "        [[0.5373, 0.6353, 0.6941,  ..., 0.9882, 0.9922, 0.9922],\n",
       "         [0.5098, 0.5176, 0.5137,  ..., 0.9882, 0.9922, 0.9922],\n",
       "         [0.4275, 0.5098, 0.4980,  ..., 0.9882, 0.9922, 0.9922],\n",
       "         ...,\n",
       "         [0.5412, 0.5294, 0.5373,  ..., 0.3608, 0.3529, 0.3333],\n",
       "         [0.5333, 0.5647, 0.5608,  ..., 0.3765, 0.3608, 0.3529],\n",
       "         [0.5255, 0.5843, 0.5529,  ..., 0.3608, 0.3451, 0.3176]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset['train'][0]\n",
    "transformed_dataset['train'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'country_iso_alpha2', 'latitude', 'longitude', 'address'],\n",
       "        num_rows: 6632\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'country_iso_alpha2', 'latitude', 'longitude', 'address'],\n",
       "        num_rows: 2211\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image', 'country_iso_alpha2', 'latitude', 'longitude', 'address'],\n",
       "        num_rows: 2211\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.4)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']\n",
    "})\n",
    "# datasets.save_to_disk(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ZA': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'KR': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'AR': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'BW': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'GR': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SK': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'HK': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NL': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PE': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'AU': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'KH': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'LT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NZ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RO': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'MY': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SG': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'AE': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'FR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'ES': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'IT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'IE': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'LV': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'IL': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'JP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CH': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'AD': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CA': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RU': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NO': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SE': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PL': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'TW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CO': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'BD': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'HU': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CL': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'IS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'BG': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'GB': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'US': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SI': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'BT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'FI': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'BE': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'EE': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SZ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'UA': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CZ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'BR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'DK': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'ID': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'MX': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'DE': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 'HR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 'PT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 'TH': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "country_codes = [\"ZA\",\"KR\",\"AR\",\"BW\",\"GR\",\"SK\",\"HK\",\"NL\",\"PE\",\"AU\",\"KH\",\"LT\",\"NZ\",\"RO\",\"MY\",\"SG\",\"AE\",\"FR\",\"ES\",\"IT\",\"IE\",\"LV\",\"IL\",\"JP\",\"CH\",\"AD\",\"CA\",\"RU\",\"NO\",\"SE\",\"PL\",\"TW\",\"CO\",\"BD\",\"HU\",\"CL\",\"IS\",\"BG\",\"GB\",\"US\",\"SI\",\"BT\",\"FI\",\"BE\",\"EE\",\"SZ\",\"UA\",\"CZ\",\"BR\",\"DK\",\"ID\",\"MX\",\"DE\",\"HR\",\"PT\",\"TH\"]\n",
    "country_dict = {}\n",
    "# TODO: these might need to be tensor arrays but thats easy enough to change if needed\n",
    "for i in range(len(country_codes)):\n",
    "    country_dict[country_codes[i]] = [0]*len(country_codes)\n",
    "    country_dict[country_codes[i]][i] = 1\n",
    "# print(country_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(datasets)\n",
    "# datasets['train'][0]['image']\n",
    "type(datasets['train'][0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced: https://blog.paperspace.com/convolutional-autoencoder/\n",
    "# autoencoder classes (CREDIT: LARGELY TAKEN FROM 6_AUTOENCODER NOTEBOOK, but encoder and decoder architectures modified to be convolutional)\n",
    "# should only have Encoder that has a latent dimension of 50 - corresponding to country weights\n",
    "\n",
    "class MLPEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 number_of_hidden_layers: int,\n",
    "                 latent_size: int,\n",
    "                 hidden_size: int,\n",
    "                 input_size: int,\n",
    "                 activation: torch.nn.Module):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        assert number_of_hidden_layers >= 0, \"Decoder number_of_hidden_layers must be at least 0\"\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        out_channels = 16\n",
    "        layers.append(torch.nn.Conv2d(in_channels, #in_channels\n",
    "                                      out_channels, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=2, #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        # 32 x 32\n",
    "        layers.append(torch.nn.Conv2d(out_channels, #in_channels\n",
    "                                      out_channels, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=1, #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        layers.append(torch.nn.Conv2d(out_channels, #in_channels\n",
    "                                      out_channels * 2, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=2,  #stride\n",
    "                                      padding=1 \n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        #16 x 16\n",
    "        layers.append(torch.nn.Conv2d(out_channels * 2, #in_channels\n",
    "                                      out_channels * 2, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=1,  #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        # 8x8 feature maps\n",
    "        # 64 out channels\n",
    "        layers.append(torch.nn.Conv2d(out_channels * 2, #in_channels\n",
    "                                      out_channels*4, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=2,  #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        # input is now 8x8 feature maps for out_channels*4 channels.\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        #flatten to latent_size \n",
    "        layers.append(torch.nn.Linear(4*out_channels*8*8 , # features in\n",
    "                                      latent_size # features out\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #return torch.rand(1,self.latent_size)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our training parameters and model\n",
    "hidden_layers = 4\n",
    "hidden_size = 30\n",
    "\n",
    "latent_size = 55\n",
    "## this might need to change\n",
    "input_size = 64\n",
    "lr = 0.001\n",
    "# lambda weight for classifier's loss\n",
    "lamb = 1\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPEncoder( number_of_hidden_layers=hidden_layers,\n",
    "                 latent_size=latent_size,\n",
    "                 hidden_size=hidden_size,\n",
    "                 input_size=input_size,\n",
    "                 activation=torch.nn.ReLU()).to(device)\n",
    "\n",
    "# use an optimizer to handle parameter updates\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# save all log data to a local directory\n",
    "run_dir = \"logs\"\n",
    "\n",
    "# to clear out TensorBoard and start totally fresh, we'll need to\n",
    "# remove old logs by deleting them from the directory\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# timestamp the logs for each run so we can sort through them\n",
    "run_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "# initialize a SummaryWriter object to handle all logging actions\n",
    "logger = SummaryWriter(log_dir=Path(run_dir) / run_time, flush_secs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from disk\n",
    "data = datasets.load_from_disk(\"./data\")\n",
    "data = data.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (***credit***: mostly taken from provided notebook )\n",
    "# training\n",
    "\n",
    "epochs = 100\n",
    "start_time = time.time()\n",
    "loss_history = []\n",
    "valid_history = []\n",
    "acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "# split this into batches\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # weight batch losses/scores proportional to batch size\n",
    "    iter_count = 0\n",
    "    valid_iter_count = 0\n",
    "    loss_epoch = 0\n",
    "    class_accuracy_epoch = 0\n",
    "    valid_loss_epoch = 0\n",
    "    valid_accuracy_epoch = 0\n",
    "    ###\n",
    "    ### IMAGE_DATA_TRAIN is training data, shape is 610 x 3 x 64 x 64\n",
    "    ### \n",
    "    \n",
    "   # print(batched_image_data_train[0][0].shape)\n",
    "\n",
    "    # test with literally only one image\n",
    "    #batched_image_data_train = [batched_image_data_train[0][0].unsqueeze(0)]\n",
    "    for idx, img in enumerate(datasets['train']):\n",
    "        \n",
    "        x = img['image']\n",
    "        # print(x)\n",
    "        # flatten input images and move to device\\\n",
    "        # *****\n",
    "        x = x / 255\n",
    "        # plot x_real later to see if this is correct\n",
    "        x = x.to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # train on a batch of inputs\n",
    "        pred_labels = model(x_real)\n",
    "\n",
    "        # get the true label\n",
    "        label = country_dict[img['country_code']]\n",
    "        loss = Loss(label, pred_labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # log loss\n",
    "        loss_epoch += loss.detach().item()\n",
    "        iter_count += 1\n",
    "\n",
    "        # apply the .5 threshold to the classification\n",
    "        true_class = int(true_label_tensor[0])\n",
    "        c = 1 if (classification.mean() > 0.5) else 0   \n",
    "    \n",
    "\n",
    "        # classification accuracy\n",
    "        acc = (true_class == c) \n",
    "        class_accuracy_epoch += acc\n",
    "\n",
    "        # print(f\"true: {true_class}\")\n",
    "        # print(f\"pred: {classification.mean()}\")\n",
    "        # print(f\"acc: {acc}\")\n",
    "\n",
    "    # plot loss\n",
    "    loss_epoch /= iter_count\n",
    "    class_accuracy_epoch /= iter_count\n",
    "   # print(iter_count)\n",
    "    logger.add_scalar(\"mse_loss\", loss_epoch, epoch)\n",
    "    loss_history.append(loss_epoch)\n",
    "    acc_history.append(class_accuracy_epoch)\n",
    " \n",
    "            \n",
    "            # logger.add_scalar(\"mse_loss_valid\", valid_loss_epoch, epoch)\n",
    "    # # plot example generated images\n",
    "    # with torch.no_grad():\n",
    "    #     reconstructed_batch = model(example_batch.reshape(batch_size, -1)).reshape(batch_size, 1, image_size, image_size)\n",
    "    #     logger.add_image(\"reconstructed_images\", make_grid(reconstructed_batch, math.floor(math.sqrt(batch_size)), title=\"Reconstructed Images\"), epoch)\n",
    "        # calculate validation loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(batched_image_data_valid):\n",
    "            x_real = batch_data.float()\n",
    "            x_real = x_real / 255\n",
    "            x_real = x_real.to(device)\n",
    "            x_reconstructed, classification = model(x_real)\n",
    "\n",
    "            true_label_tensor = torch.tensor(labels['valid'][batch_idx]).repeat(5,1).float()\n",
    "            valid_loss = loss([x_real,classification], [x_reconstructed, true_label_tensor])\n",
    "            valid_loss_epoch += valid_loss.detach().item() * valid_batch_size\n",
    "\n",
    "            \n",
    "                # apply the .5 threshold to the classification\n",
    "            true_class = int(true_label_tensor[0])\n",
    "            c = 1 if (classification.mean() > 0.5) else 0   \n",
    "\n",
    "            \n",
    "\n",
    "            # classification accuracy\n",
    "            valid_acc = (true_class == c) * valid_batch_size\n",
    "            valid_accuracy_epoch += valid_acc\n",
    "            valid_iter_count += valid_batch_size\n",
    "            # print(f\"true: {true_class}\")\n",
    "            # print(f\"pred: {classification.mean()}\")\n",
    "            # print(f\"acc: {valid_acc}\")\n",
    "        valid_loss_epoch /= valid_iter_count\n",
    "        valid_history.append(valid_loss_epoch)\n",
    "        valid_accuracy_epoch /= valid_iter_count\n",
    "        valid_acc_history.append(valid_accuracy_epoch)\n",
    "\n",
    "    if (epoch + 1) % report_every == 0:\n",
    "        mins = (time.time() - start_time) / 60\n",
    "        print(f\"Epoch: {epoch + 1:5d}\\tMSE Loss: {loss_epoch :6.4f}\\t in {mins:5.1f}min\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
