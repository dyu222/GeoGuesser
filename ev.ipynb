{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from datasets import DatasetDict\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 6.85k/6.85k [00:00<00:00, 2.46MB/s]\n",
      "Downloading metadata: 100%|██████████| 974/974 [00:00<00:00, 1.29MB/s]\n",
      "Downloading data: 100%|██████████| 472M/472M [00:24<00:00, 19.1MB/s] \n",
      "Downloading data: 100%|██████████| 470M/470M [00:25<00:00, 18.6MB/s] \n",
      "Downloading data: 100%|██████████| 469M/469M [00:27<00:00, 17.3MB/s] \n",
      "Downloading data: 100%|██████████| 466M/466M [00:24<00:00, 19.0MB/s] \n",
      "Downloading data: 100%|██████████| 467M/467M [00:26<00:00, 17.6MB/s] \n",
      "Downloading data: 100%|██████████| 465M/465M [00:25<00:00, 18.5MB/s] \n",
      "Generating train split: 100%|██████████| 11054/11054 [00:26<00:00, 413.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stochastic/random_streetview_images_pano_v0.0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.4)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']\n",
    "})\n",
    "\n",
    "# TODO: need to do the train/test/validate split here\n",
    "\n",
    "# want image_data_train, image_data_test, image_data_validate\n",
    "\n",
    "# len(dataset['train'])\n",
    "# dataset['train'][2]['image']\n",
    "# filtered_data = dataset.filter(lambda example: \"x\" in example[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = [\"ZA\",\"KR\",\"AR\",\"BW\",\"GR\",\"SK\",\"HK\",\"NL\",\"PE\",\"AU\",\"KH\",\"LT\",\"NZ\",\"RO\",\"MY\",\"SG\",\"AE\",\"FR\",\"ES\",\"IT\",\"IE\",\"LV\",\"IL\",\"JP\",\"CH\",\"AD\",\"CA\",\"RU\",\"NO\",\"SE\",\"PL\",\"TW\",\"CO\",\"BD\",\"HU\",\"CL\",\"IS\",\"BG\",\"GB\",\"US\",\"SI\",\"BT\",\"FI\",\"BE\",\"EE\",\"SZ\",\"UA\",\"CZ\",\"BR\",\"DK\",\"ID\",\"MX\",\"DE\",\"HR\",\"PT\",\"TH\"]\n",
    "country_dict = {}\n",
    "for i in range(len(country_codes)):\n",
    "    country_dict[country_codes[i]] = i\n",
    "    \n",
    "def country_to_label(country):\n",
    "    label = [0]*len(country_codes)\n",
    "    label[country_dict[country]] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced: https://blog.paperspace.com/convolutional-autoencoder/\n",
    "# autoencoder classes (CREDIT: LARGELY TAKEN FROM 6_AUTOENCODER NOTEBOOK, but encoder and decoder architectures modified to be convolutional)\n",
    "# should only have Encoder that has a latent dimension of 50 - corresponding to country weights\n",
    "\n",
    "class MLPEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 number_of_hidden_layers: int,\n",
    "                 latent_size: int,\n",
    "                 hidden_size: int,\n",
    "                 input_size: int,\n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP decoder\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        assert number_of_hidden_layers >= 0, \"Decoder number_of_hidden_layers must be at least 0\"\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        out_channels = 16\n",
    "        layers.append(torch.nn.Conv2d(in_channels, #in_channels\n",
    "                                      out_channels, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=2, #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        # 32 x 32\n",
    "        layers.append(torch.nn.Conv2d(out_channels, #in_channels\n",
    "                                      out_channels, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=1, #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        layers.append(torch.nn.Conv2d(out_channels, #in_channels\n",
    "                                      out_channels * 2, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=2,  #stride\n",
    "                                      padding=1 \n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        #16 x 16\n",
    "        layers.append(torch.nn.Conv2d(out_channels * 2, #in_channels\n",
    "                                      out_channels * 2, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=1,  #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        # 8x8 feature maps\n",
    "        # 64 out channels\n",
    "        layers.append(torch.nn.Conv2d(out_channels * 2, #in_channels\n",
    "                                      out_channels*4, #out_channels\n",
    "                                      3, #kernel_size\n",
    "                                      stride=2,  #stride \n",
    "                                      padding=1\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        # input is now 8x8 feature maps for out_channels*4 channels.\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        #flatten to latent_size \n",
    "        layers.append(torch.nn.Linear(4*out_channels*8*8 , # features in\n",
    "                                      latent_size # features out\n",
    "                                      ))\n",
    "        layers.append(activation)\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #return torch.rand(1,self.latent_size)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our training parameters and model\n",
    "hidden_layers = 4\n",
    "hidden_size = 30\n",
    "\n",
    "\n",
    "latent_size = 50\n",
    "\n",
    "## this might need to change\n",
    "input_size = 64\n",
    "lr = 0.001\n",
    "# lambda weight for classifier's loss\n",
    "lamb = 1\n",
    "\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPEncoder( number_of_hidden_layers=hidden_layers,\n",
    "                 latent_size=latent_size,\n",
    "                 hidden_size=hidden_size,\n",
    "                 input_size=input_size,\n",
    "                 activation=torch.nn.ReLU()).to(device)\n",
    "\n",
    "# use an optimizer to handle parameter updates\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# save all log data to a local directory\n",
    "run_dir = \"logs\"\n",
    "\n",
    "# to clear out TensorBoard and start totally fresh, we'll need to\n",
    "# remove old logs by deleting them from the directory\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# timestamp the logs for each run so we can sort through them\n",
    "run_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "# initialize a SummaryWriter object to handle all logging actions\n",
    "logger = SummaryWriter(log_dir=Path(run_dir) / run_time, flush_secs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (***credit***: mostly taken from provided notebook )\n",
    "# training\n",
    "calc_valid = False\n",
    "epochs = 100\n",
    "# batch_size = 0.2\n",
    "# each batch is now going to go over one complete image with its adversarial examples\n",
    "train_batch_size = 5/len(image_data_train)\n",
    "valid_batch_size = 5/len(image_data_valid)\n",
    "test_batch_size = len()\n",
    "report_every = 1\n",
    "start_time = time.time()\n",
    "loss_history = []\n",
    "valid_history = []\n",
    "acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "# split this into batches\n",
    "batched_image_data_train = torch.split(image_data_train, int(train_batch_size * len(image_data_train)))\n",
    "batched_image_data_valid = torch.split(image_data_valid, int(valid_batch_size * len(image_data_valid)))\n",
    "batched_image_data_test = torch.split(image_data_test, int(test_batch_size * len(image_data_test)))\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # weight batch losses/scores proportional to batch size\n",
    "    iter_count = 0\n",
    "    valid_iter_count = 0\n",
    "    loss_epoch = 0\n",
    "    class_accuracy_epoch = 0\n",
    "    valid_loss_epoch = 0\n",
    "    valid_accuracy_epoch = 0\n",
    "    ###\n",
    "    ### IMAGE_DATA_TRAIN is training data, shape is 610 x 3 x 64 x 64\n",
    "    ### \n",
    "    \n",
    "   # print(batched_image_data_train[0][0].shape)\n",
    "\n",
    "    # test with literally only one image\n",
    "    #batched_image_data_train = [batched_image_data_train[0][0].unsqueeze(0)]\n",
    "    for batch_idx, batch_data in enumerate(batched_image_data_train):\n",
    "\n",
    "        # update the model that classifies the emoji\n",
    "        # how to know which emoji we are currently trying to classify\n",
    "        # batch index?\n",
    "        # classification is made below\n",
    "        # need to implement backprop for the model to update?\n",
    "\n",
    "        # we only care about inputs, not labels\n",
    "        x_real = batch_data.float()\n",
    "       \n",
    "        # print(x_real)\n",
    "        # flatten input images and move to device\\\n",
    "        # *****\n",
    "        x_real = x_real / 255\n",
    "        # plot x_real later to see if this is correct\n",
    "        x_real = x_real.to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # train on a batch of inputs\n",
    "        x_reconstructed, classification = model(x_real)\n",
    "\n",
    "       \n",
    "\n",
    "        true_label_tensor = torch.tensor(labels['train'][batch_idx]).repeat(5,1).float()\n",
    "        loss_batch = loss([x_real,classification], [x_reconstructed, true_label_tensor])\n",
    "        loss_batch.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # log loss\n",
    "        loss_epoch += loss_batch.detach().item() * train_batch_size\n",
    "        iter_count += train_batch_size\n",
    "\n",
    "        # apply the .5 threshold to the classification\n",
    "        true_class = int(true_label_tensor[0])\n",
    "        c = 1 if (classification.mean() > 0.5) else 0   \n",
    "    \n",
    "\n",
    "        # classification accuracy\n",
    "        acc = (true_class == c) * train_batch_size\n",
    "        class_accuracy_epoch += acc\n",
    "\n",
    "        # print(f\"true: {true_class}\")\n",
    "        # print(f\"pred: {classification.mean()}\")\n",
    "        # print(f\"acc: {acc}\")\n",
    "\n",
    "    # plot loss\n",
    "    loss_epoch /= iter_count\n",
    "    class_accuracy_epoch /= iter_count\n",
    "   # print(iter_count)\n",
    "    logger.add_scalar(\"mse_loss\", loss_epoch, epoch)\n",
    "    loss_history.append(loss_epoch)\n",
    "    acc_history.append(class_accuracy_epoch)\n",
    " \n",
    "            \n",
    "            # logger.add_scalar(\"mse_loss_valid\", valid_loss_epoch, epoch)\n",
    "    # # plot example generated images\n",
    "    # with torch.no_grad():\n",
    "    #     reconstructed_batch = model(example_batch.reshape(batch_size, -1)).reshape(batch_size, 1, image_size, image_size)\n",
    "    #     logger.add_image(\"reconstructed_images\", make_grid(reconstructed_batch, math.floor(math.sqrt(batch_size)), title=\"Reconstructed Images\"), epoch)\n",
    "        # calculate validation loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(batched_image_data_valid):\n",
    "            x_real = batch_data.float()\n",
    "            x_real = x_real / 255\n",
    "            x_real = x_real.to(device)\n",
    "            x_reconstructed, classification = model(x_real)\n",
    "\n",
    "            true_label_tensor = torch.tensor(labels['valid'][batch_idx]).repeat(5,1).float()\n",
    "            valid_loss = loss([x_real,classification], [x_reconstructed, true_label_tensor])\n",
    "            valid_loss_epoch += valid_loss.detach().item() * valid_batch_size\n",
    "\n",
    "            \n",
    "                # apply the .5 threshold to the classification\n",
    "            true_class = int(true_label_tensor[0])\n",
    "            c = 1 if (classification.mean() > 0.5) else 0   \n",
    "\n",
    "            \n",
    "\n",
    "            # classification accuracy\n",
    "            valid_acc = (true_class == c) * valid_batch_size\n",
    "            valid_accuracy_epoch += valid_acc\n",
    "            valid_iter_count += valid_batch_size\n",
    "            # print(f\"true: {true_class}\")\n",
    "            # print(f\"pred: {classification.mean()}\")\n",
    "            # print(f\"acc: {valid_acc}\")\n",
    "        valid_loss_epoch /= valid_iter_count\n",
    "        valid_history.append(valid_loss_epoch)\n",
    "        valid_accuracy_epoch /= valid_iter_count\n",
    "        valid_acc_history.append(valid_accuracy_epoch)\n",
    "\n",
    "    if (epoch + 1) % report_every == 0:\n",
    "        mins = (time.time() - start_time) / 60\n",
    "        print(f\"Epoch: {epoch + 1:5d}\\tMSE Loss: {loss_epoch :6.4f}\\t in {mins:5.1f}min\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
